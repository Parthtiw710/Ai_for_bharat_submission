# ğŸ§  Feedback Intelligence

**Structured ingestion â†’ Hybrid analysis â†’ Actionable insights**

Transforms unstructured user feedback into decision-ready intelligence through deterministic analytics and LLM-powered reasoning.

---

## âš¡ System Architecture

```
Widget (JSON Schema) â†’ Unified API â†’ PostgreSQL (JSONB) â†’ AI Agent â†’ Insights
```

**Stack**: Node.js Â· Express Â· PostgreSQL Â· OpenAI/Anthropic Â· Docker

**Components**: 2 services + 1 database

---

## ğŸ§© Schema-Driven Widget

Dynamic form rendering from JSON configurationâ€”zero redeployments:

```javascript
{
  "fields": [
    { "type": "rating", "id": "satisfaction", "max": 5 },
    { "type": "text", "id": "feedback", "required": true },
    { "type": "enum", "id": "category", "options": ["bug", "feature"] }
  ]
}
```

**Vanilla JS** Â· Single `<script>` embed Â· Platform-agnostic

---

## ğŸ—„ï¸ Hybrid Data Model

PostgreSQL with JSONB for relational integrity + document flexibility:

```sql
CREATE TABLE submissions (
  id UUID PRIMARY KEY,
  feedback_text TEXT NOT NULL,
  user_context JSONB,  -- Flexible metadata
  status VARCHAR(20),
  batch_id UUID REFERENCES batches(id)
);

CREATE INDEX idx_context ON submissions USING GIN (user_context);
```

**ACID transactions** Â· **GIN indexes** for JSON queries Â· **Foreign key constraints**

---

## ğŸ”¬ Hybrid Intelligence Engine

### Deterministic Layer
- Frequency distribution & temporal aggregation
- Statistical sentiment scoring
- Pattern-based urgency detection

### AI Layer (GPT-4o-mini / Claude 3.5 Haiku)
- Semantic theme clustering
- Intent extraction & urgency classification
- Prioritized recommendation synthesis

**Token-aware batching**: 6K limit Â· 4:1 char-to-token estimation Â· Async processing

---

## ï¿½ Data Flow

```mermaid
graph LR
    A[Widget] --> B[POST /feedback]
    B --> C[(PostgreSQL)]
    C --> D[Preprocessing]
    D --> E[AI Agent]
    E --> F[LLM API]
    F --> E
    E --> C
    C --> G[GET /insights]
```

**Async by default**: Instant submission response Â· Background processing Â· Manual trigger via `/process`

---

## ğŸ¯ API Endpoints

```bash
POST   /api/v1/feedback      # Submit feedback
GET    /api/v1/feedback      # Query submissions (paginated)
GET    /api/v1/insights      # Retrieve AI analysis
POST   /api/v1/process       # Manual pipeline trigger
GET    /api/v1/health        # Health check
```

---

## ğŸš€ Technical Implementation

| Component | Details |
|-----------|---------|
| **Widget** | Vanilla JS Â· 12KB gzipped Â· Client-side validation |
| **Unified API** | Express Â· Connection pool (10) Â· Node-cron scheduler |
| **Preprocessing** | Idempotent batching Â· Rollback on failure Â· Status tracking |
| **AI Agent** | Direct LLM calls Â· JSON mode Â· 2-retry backoff |
| **Database** | PostgreSQL 15 Â· JSONB + relational Â· GIN indexes |

---

## ğŸ—ï¸ Quick Start

```bash
git clone <repo-url> && cd feedback-intelligence
cp .env.example .env  # Add LLM_API_KEY
docker-compose up -d
psql $DATABASE_URL < migrations/001_create_tables.sql
curl http://localhost:3000/api/v1/health
```

**Services**: API (`:3000`) Â· AI Agent (`:3001`) Â· PostgreSQL (`:5432`)

---

## ğŸ“ˆ Performance

- **Ingestion**: <100ms (p95)
- **Batch processing**: 5-min intervals
- **AI analysis**: 10-30s per batch
- **Throughput**: 50+ req/s (single instance)

---

**Built for hackathons. Ready for production.**

*Feedback is abundant. Intelligence is rare.*
